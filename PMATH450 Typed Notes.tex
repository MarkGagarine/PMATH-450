

% This is a .tex file which can serve as model (if you want) 
% for the formatting of your PMath 351 Lecture Shell X.
%
% Note: in this file, the lines that start with the % symbol are
% just for comments, and are not read by the LaTeX compiler.
% 

\documentclass[11pt]{amsart}

\setlength{\textwidth}{6in} 
\setlength{\textheight}{8.7in}
\setlength{\oddsidemargin}{.3in}
\setlength{\evensidemargin}{0.3in}
\setlength{\topmargin}{-.25in}

\usepackage{amssymb, amsfonts, amsmath, amsthm, latexsym, xcolor, hyperref, pifont, graphicx,enumitem}

\pagestyle{headings}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{prop-and-def}[theorem]{Proposition and Definition}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition-and-remark}[theorem]{Definition and Remark}
\newtheorem{remark-and-notation}[theorem]{Remark and Notation}
\newtheorem{notation-and-remark}[theorem]{Notation and Remark}
\newtheorem{example}[theorem]{Example}


\numberwithin{equation}{section}


% \newcommand: gives some abbreviations which you can use 
% while you type.

\newcommand{\cA}{ \mathcal{A} }
\newcommand{\cB}{ \mathcal{B} }
\newcommand{\cT}{ \mathcal{T} }

\newcommand{\fM}{ \mathfrak{M} }
\newcommand{\fT}{ \mathfrak{T} }

\newcommand{\bN}{ \mathbb{N} }	% Natural Numbers
\newcommand{\bQ}{ \mathbb{Q} }	% Rational Numbers
\newcommand{\bR}{ \mathbb{R} }	% Real Numbers
\newcommand{\bC}{ \mathbb{C} }	% Complex Numbers
\newcommand{\bZ}{ \mathbb{Z} }	% Integers

\newcommand{\ee}{ \varepsilon }
\newcommand{\dusual}{ d_{\mathrm{usual}} }					% Usual distance
\newcommand{\ips}{ \left( X, \langle \cdot, \cdot \rangle \right) }		% Inner product space
\newcommand{\nextgen}{ C\left( [a,b], \bR\right) }				% "Next Gen. space"
\newcommand{\nvs}{\left(X, \| \cdot \| \right)}					% Normed Vector space
\newcommand{\ms}{\left(X, d \right)}							% Metric space
\newcommand{\proj}{ P_W(x_0) } 							% Orthogonal projection of x_0 onto W
\newcommand{ \sspan }{ \mbox{span} }						% Span of S
\newcommand{ \clspan }{ \mbox{clspan} }						% Closure of Span(S)
\newcommand{ \seq }{ (x_n)_{n=1}^\infty }						% Sequence of x_n \in X from 1 to \infty
\newcommand{ \cl }{ \mbox{cl} }							% Closure of a set
\newcommand{ \incrchn }{ X_1 \subseteq \cdots X_n \subseteq \cdots } % increasing chain of linear subspaces
\newcommand{ \onseq }{(\xi_n)^\infty_{n=1}} 					% orthonormal sequence



\title[PMath 450 -- Summer 2024]{PMATH 450: Lebesgue Integration and Fourier Analysis} 


\author[M. Gagarine]{Mark Gagarine}
\address{Faculty of Mathematics,
University of Waterloo} 
\email{magagri@uwaterloo.ca}


\begin{document}

\begin{abstract}
This is a second course in real analysis, coming in the continuation of PMath 351. There are two main strands of the course: one of them concerns Hilbert spaces and elements of Fourier series for periodic functions (the so-called “harmonic analysis on
the circle”). The other strand concerns the study of the Lebesgue measure on the real line and of the Lebesgue integral, which are then used in the treatment of the aforementioned
harmonic analysis developments. 
\end{abstract}

\maketitle

\tableofcontents

\newpage

\part{Hilbert Space}

\section{Inner Product Space, Hilbert Space}

\subsection{May 6, Lecture 1}

\begin{definition}\label{def:11}
Let $X$ be a vector space over $\bR$ (could also be $\bC$). The \textit{inner product} over $X$ is a rule that assigns values $\langle x, y \rangle \in \bR, \forall x,y \in X$, which satisfies the following rules. 
\begin{description}
\item[Bilinearity]
\vspace{6pt}

\[\label{eq:Bi-Lin 1} \tag{Bi-Lin 1} \langle \alpha_1 x_1 + \alpha_2 x_2, y \rangle = \alpha_1 \langle x_1, y\rangle + \alpha_2 \langle x_2, y \rangle , \forall x_1, x_2, y \in X, \alpha_1, \alpha_2 \in \bR\]

\[\label{eq:Bi-Lin 2} \tag{Bi-Lin 2} \langle x,  \beta_1 y_1 + \beta_2 y_2 \rangle = \beta_1 \langle x, y_1 \rangle + \beta_2 \langle x, y_2 \rangle , \forall x, y_1, y_2 \in X, \beta_1, \beta_2 \in \bR \]

\vspace{6pt}

\item[Symmetry] \[\label{eq:Sym} \tag{Sym} \langle x,y \rangle = \langle y,x \rangle, \forall x,y \in X \]

\vspace{6pt}

\item[Positive Definite] \[ \label{eq:Pos Def} \tag{Pos Def} \langle x, x \rangle	> 0, \forall x \in X\]
\end{description}

\vspace{6pt}

The pair $\left( X, \langle \cdot, \cdot \rangle \right)$ is called an \textit{inner product space}  (ips).
\end{definition}

\vspace{6pt}

\begin{remark}\label{rem:12}
\begin{enumerate}

\item \textbf{Redundancy in (Bi-Lin):} ~\ref{eq:Bi-Lin 1} + ~\ref{eq:Sym} $\Rightarrow$ ~\ref{eq:Bi-Lin 2}.

\vspace{6pt}

\item \textbf{Rephrasing of (Pos Def):} Observe that $\langle 0_X, y \rangle = \langle y, 0_X \rangle, \forall y \in X$. \textit{Why?} 

$$2 \cdot 0_X = 0_X \Rightarrow \langle 2 \cdot 0_X, y \rangle = \langle 0_X, y \rangle \Leftrightarrow 2 \langle 0_X, y \rangle = \langle 0_X, y \rangle \Rightarrow  \langle 0_X, y \rangle = 0 $$

This allows us to rephrase ~\ref{eq:Pos Def} as

\[\label{eq:Pos Def prime} \tag{Pos Def$^{\prime}$} \langle x, x \rangle \ge 0, \forall x \in X, \mbox{ with equality iff } x = 0_X.\]
\end{enumerate}

This technique of checking if the inner product is 0 is a useful trick for proving identities. 
\end{remark}

\vspace{6pt}

\begin{notation}\label{not:13}
Let $\ips$ be an inner product space. For all $x \in X$, denote

 $$||x|| = \sqrt{\langle x, x \rangle} \in [0, \infty)$$
\end{notation}

\vspace{6pt}

\begin{proposition}\label{prop:14} Let $\ips$ be an inner product space.

\begin{enumerate}
\vspace{6pt}
\item \textbf{Cauchy-Schwarz Inequality:}

\[\label{eq:C-S} \tag{C-S} | \langle x,y \rangle | \le ||x|| \cdot ||y||, \forall x,y \in X. \]

\vspace{6pt}

Moreover, ~\ref{eq:C-S} holds with equality iff $x,y$ are dependent. That is, either one of $x = 0_X$,  $y = 0_X$, or $x,y$ are dependent. That is, $\exists \alpha \in \bR \mbox{ such that } x = \alpha y$. 

\vspace{6pt}

\item \textbf{Norm:} The function $||\cdot || : X \rightarrow \bR$ in Notation ~\ref{not:13} is a norm on X. 

\end{enumerate}
\end{proposition}

\begin{proof}
...
\end{proof}

\vspace{6pt}

\begin{definition}[Hilbert Space]	\label{def:15}
Let $\ips$ be an inner product space. When viewed as a normed vector space $\left( X, ||\cdot || \right)$ (hence a metric space $\left( X, d \right)$ with $d(x,y) = || x - y ||$), if $X$ is complete wrt d, then $\ips$ is a \textit{Hilbert Space}. 
\end{definition}

This comes from the fact that a complete normed vector space is a Banach space, so a Hilbert space is simply a collection within a Banach space. 

\vspace{6pt}

\begin{example}[A Hilbert Space]\label{ex:16}
Consider $X = \bR^k$ with the \textit{standard inner product}. That is, for $x = \left(x^{(1)}, \cdots, x^{(k)}\right), \langle x,y \rangle 
= \sqrt{x^{(1)}y^{(1)} + \cdots + x^{(k)}y^{(k)} } $. We get $||x|| 
= \sqrt{\left(x^{(1)}\right)^2 + \cdots + \left(x^{(1)}\right)^2}	$.
\end{example}

\vspace{6pt}

\begin{example}[Not a Hilbert Space]\label{ex:17}

$$\mbox{\textbf{Recall:} } X = c_{00} = \left\{ x = \left(x^{(1)}, \cdots, x^{(k)}, \cdots \right) | \; \exists k_0 \in \bN \mbox{ s.t. } x^{(k)} = 0,\\ \forall k > k_0\right\}. $$

For $x,y \in X$, set $\langle x,y \rangle = \sum_{k = 1}^{\infty} x^{(k)}y^{(k)} = \sum_{k = 1}^{k_0} x^{(k)}y^{(k)}$. It's easy to see $\left(c_{00}, \langle \cdot, \cdot \rangle \right)$ is an inner product space. We denote the norm on $c_{00}$ associated to $\langle \cdot, \cdot \rangle$ as $|| \cdot ||_2$,  

$$||x||_2 = \sqrt{ \langle x,x\rangle} = \sqrt{ \sum_{k = 1}^{\infty}\left[x^{(k)}\right]^2}$$
\vspace{6pt}

\textbf{However}, we can find a cauchy sequence in $c_{00}$ that doesn't converge wrt $||\cdot||_2$. Hence $ \left(c_{00}, ||\cdot ||_2\right) $ isn't complete, so $ \left(c_{00}, \langle \cdot , \cdot \rangle \right) $ is not a Hilbert space. 
\end{example}

\vspace{6pt}

\subsection{May 8, Lecture 2}
\textit{How do we get a Hilbert space?} \\ 

Recall $ \left(c_{00}, ||\cdot ||_2\right) $ in Example ~\ref{ex:17} isn't complete, so how do we get a Hilbert space? We can \textit{complete} $ \left(c_{00}, ||\cdot ||_2\right) $. That is, we embed $c_{00}$ into a larger complete normed vector space $Z$, such that $c_{00}$ is dense in $Z$. 

\vspace{6pt}

Denote  $\ell^2 = \left\{ x = \left(x^{(1)}, \cdots, x^{(k)}, \cdots \right) |  \sum_{k = 1}^{\infty}\left[x^{(k)}\right]^2 < \infty \right\} \Leftrightarrow \sup \left\{  \sum_{k = 1}^{n}\left[x^{(k)}\right]^2 | n \in \bN \right\}$. 

\vspace{6pt}

The following is a series of claims showing $\ell^2$ is complete, proved in Assignment 1 Q1-3. %\footnote{These were proved in Homework Assignment 1}  

\begin{proof}[Claim 1] \textit{For $x,y \in \ell^2$, $\sum_{k=1}^{\infty} x^{(k)} y^{(k)}$ converges absolutely.}
...
\end{proof}

So we can put $\langle x,y \rangle = \sum_{k=1}^{\infty} x^{(k)} y^{(k)} = \lim_{n \rightarrow \infty} \sum_{k=1}^{n} x^{(k)} y^{(k)}$. 

\begin{proof}[Claim 2] \textit{$\left(\ell^2, \langle \cdot, \cdot \rangle \right)$ is an inner product space}
...
\end{proof}

\begin{proof}[Claim 3] \textit{$\ell_2$ is complete wrt $||\cdot ||_2$}
...
\end{proof}

So $\left( \ell^2 , \langle \cdot, \cdot \rangle \right)$ is a Hilbert space. We can also see that $c_{00} \subseteq \ell^2$, leading to the final claim. 

\begin{proof}[Claim 4] \textit{$c_{00}$ is dense in $\ell^2$ (wrt $||\cdot ||$)}
...
\end{proof}

\begin{example}\label{ex:18} Pick $a < b \in \bR$, and consider the vector space $$\nextgen = \left\{ f:[a,b]\rightarrow\bR \; | \; f \mbox{ is continuous} 	\right\}$$ 

\textbf{We saw in PMATH 351:} $\left( \nextgen, ||\cdot ||_{\infty} \right)$ is a Banach space with 

\[\label{eq:Sup-Norm} \tag{Sup-Norm} ||f||_{\infty} = \sup \{ |f(x)| \; | \; x\in [a,b]	\} \]
\newpage 
\textbf{In PMATH 450:} For $f,g \in \nextgen$, we denote 

$$\langle f,g \rangle = \int_a^b f(x)g(x)dx \in \bR.$$

\begin{proof}[Claim] \textit{$\left(\nextgen, \langle \cdot, \cdot \rangle	\right)$ is an inner product space.}
Observe $\langle f, f\rangle = \int_a^b [f(x)]^2 dx$. 

Verifying the inner product space axioms is straight forward, but special attention is needed for $\langle f, f \rangle = 0$ That is, when $f$ is the zero function in $\nextgen$, denoted \\ $\underline 0: [a,b] \rightarrow \bR$, where $\underline 0 (x) = 0, \forall x \in [a,b]$.

\textbf{Recall:} (~\ref{eq:Pos Def prime}) states $\langle x, x \rangle \ge 0$ with equality iff $x = 0_X$. So we need to check 

$$ f(x) \mbox{ continuous}, \int_a^b [f(x)]^2 dx \Rightarrow f(x) = 0, \forall x \in [a,b] $$
\end{proof}

So we get that $\left( \nextgen, \langle \cdot, \cdot \rangle \right)$ is an inner product space. The associated norm is $\| \cdot \|_2$, 

$$ \| f \|_2 = \sqrt{ \langle f , f \rangle } = \sqrt{\int_a^b [f(x)]^2dx}$$

\end{example}

\vspace{10pt}

\section{Some Hilbert Space Geometry: \textit{Distance to a closed convex set}}

\vspace{6pt}

\subsection{May 10, Lecture 3} \textit{Recap of convex sets and some metrics}

\begin{definition}\label{def:21} Let $X$ be a vector space over $\bR$. 
\begin{enumerate}
\item The \textit{line segment} between two points $x,y$ in $X$ is denoted by

\[\label{eq:Lin-Seg} \tag{Lin-Seg} Co(x,y) = \{tx + (1-t)y \; | \; t \in [0,1] \} \]

\vspace{6pt}

\item The set $A \subseteq X$ is said to be \textit{convex} if the following is satisfied. 

\[\label{eq:Convex} \tag{Convex} x,y \in A \Rightarrow Co(x,y) \subseteq A \]
\end{enumerate}
\end{definition}

\vspace{6pt}

\begin{remark}\label{rem:22} 
\begin{enumerate}
\item Let $\nvs$ be a normed vector space over $\bR$. \\ If $A \subseteq X$ is convex, then Cl$(A)$ is also convex. 

\vspace{6pt}

\item Let $\ms$ be a metric space, and let $A \subseteq X$ be a closed non-empty set. \\ 
\textbf{Recall, } in PMATH 351 we defined the \textit{distance-to-$A$} function $d_A: X \rightarrow \bR$,

\[\label{eq:dist-to-A} \tag{dist-to-$A$} d_A(x) = \inf \{d(x,a) \; | \; a \in A \} \]
\end{enumerate}
\end{remark}

Some properties associated with this function included  
\begin{itemize}
\item $d_A(x) \ge 0 \forall x \in X$, with $d_A(x) = 0 \Leftrightarrow x \in A$. 

\item $d_A$ is continuous, and contractive to mean

\[ |d_A(x) - d_A(y)| \le d(x,y), \; \forall x,y \in X \]
\end{itemize}

Combining these facts, we introduce the first theorem of the course. 

\begin{theorem} \label{thm:23}
Let $\ips$ be a Hilbert space. Suppose we're given a closed, convex, non-empty set $A \subseteq X$, and the point $x_0 \in X$.

 Then, there exists a unique point $a_0 \in A$ such that 
 
 \[ \| x_0 - a_0 \| = \inf \{ \| x_0 - a \| \; | \; a \in A \} = d_A(x_0) \]
\end{theorem}

\vspace{6pt}

\begin{proof} The case where $x_0 \in A$ is clear. Indeed, if $x_0 \in A$, then $d_A(x_0) = 0$, and the unique point $a_0 \in A$ is $x_0$. So for the rest of the proof assume $x_0 \not \in A$, so $d_A(x_0) > 0$. 

\vspace{6pt}

Denote $\alpha = d_A(x_0) = \inf \{ \| x_0 - a \| \: | \; a \in A \}$. It suffices to show $\exists a_0 \in A$ uniquely determined such that $\| x_0 - a_0 \| = \alpha$. \\

\noindent \textit{Proof of Uniqueness}\footnote{We leverage question 5c from Homework 1}

\vspace{6pt}

Suppose for a contradiction that we have two distinct points $a_1 \not = a_2 \in A$ such that $\| x_0 - a_1 \| = \| x_0 - a_2 \| = \alpha$. Denote the distance between these points as $\beta = \| a_1 - a_2 \|$. Note that $\beta > 0$ since $a_1 \not = a_2$. 

\vspace{6pt}

Let $a_3 = \frac 1 2 (a_1 + a_2)$ be the midpoint between our points $a_1$ and $a_2$, and observe $a_3 \in A$ since $A$ is assumed to be convex. Question 5c says $\| x_0 - a_3 \| = \sqrt{ \alpha^2 - \frac{\beta^2} 4 }$

\vspace{6pt}

So then $\| x_0 - a_3 \| < \alpha = \inf \{ \| x_0 - a \| \; | \; a \in A \}$. {\color{red} A contradiction} since the definition of $\alpha$ forces $\| x_0 - a \| \ge \alpha, \forall a \in A$. \end{proof}

\vspace{10pt}

\subsection{May 13, Lecture 4} We start by completing the proof of Theorem ~\ref{thm:23}. 

\begin{proof}[Proof of Existence of $a_0$] Moving along with the proof we take not of a few facts. 

\[ \tag{$\diamondsuit$} \| x_0 - a \| \ge \alpha , \forall a \in A \]

\[ \tag{$\diamondsuit \diamondsuit$} \forall n \in \bN, \exists a_n \in A \mbox{ s.t } \| x_0 - a_n \| < \alpha + \frac 1 n. \]

From $( \diamondsuit \diamondsuit)$ we get a sequence $(a_n)_{n = 1}^{\infty}$ of points in $A$. The following are a list of claims finishing off the proof. 

\textit{Claim 1.} $\forall m,n \in \bN$ we have 
\begin{eqnarray}
\| a_n - a_m \| ^2 & \le & 4\alpha \left( \frac 1 m + \frac 1 n \right) + \left( \frac 2 {m^2} + \frac 2 {n^2} \right)  \nonumber \\
			& \le & 4 \alpha \left( \frac 1 m + \frac 1 n \right) + \left( \frac 2 m + \frac 2 n \right) \nonumber  \\
			& = & (4 \alpha + 2) \left( \frac 1 m + \frac 1 n \right) \nonumber 
\end{eqnarray}
\newpage
\textit{Verification.} Write $\|a_m - a_n\|^2 = \|(x_0 - a_n) - (x_0 - a_m)\|^2 $, and by (Par Law), 
\[ \tag{\ding{37}} = 2 \cdot \|x_0 - a_m \|^2 + 2\cdot \| x_0 - a_n \|^2 -  \|(x_0 - a_n) - (x_0 - a_m)\|^2 \]
\[ \le 2 \left( \alpha + \frac 1 m \right)^2 + 2 \left( \alpha +\frac 1 n \right)^2 - 4 \alpha ^2\]

For the $3^{rd}$ term in \ding{37}:
\begin{align*}
  \|(x_0 - a_n) - (x_0 - a_m)\|^2 &= \|2 x_0 - (a_n + a_m) \|^2 \\
  &= \|2 \left(x_0 + \frac 1 2 (a_n + a_m) \right)\|^2	\\
  &= \tag{$\diamondsuit$} 2^2 \|x_0 + \frac 1 2 (a_m + a_n) \|^2 \ge 4 \alpha^2
\end{align*}

This gives us \ding{37} is bounded by $4 \alpha \left( \frac 1 m +  \frac 1 n \right) + \left( \frac 1 {m^2} +  \frac 1 {n^2} \right)$, so we are done with \textit{Claim 1}. 

\vspace{6pt}

\textit{Claim 2.} We now claim that $(a_n)^\infty_{n = 1}$ is a cauchy sequence. 

\vspace{6pt}

\textit{Verification.} Let $\epsilon > 0$ be given. We wish to find an $n_0 \in \bN$ such that
\[\  m,n \ge n_0 \Rightarrow \| a_m - a_n \| < \epsilon. \]

Pick $n_0$ such that $\frac{8 \alpha + 4} {n_0} < \epsilon^2$. Then $\forall m,n \ge n_0$, 
\[ \|a_m - a_n \|^2 \le (4\alpha + 2) \left( \frac 1 m +  \frac 1 n \right) \le (4\alpha + 2) \left( \frac 1 {m_0} +  \frac 1 {n_0} \right) \]

\[ = \frac{8 \alpha + 4} {n_0} < \epsilon^2\]

Hence $\|x_n - x_m \| < \epsilon, \forall m,n \ge n_0$ as required, and we are done with \textit{Claim 2}. 

\vspace{6pt}

\textit{Claim 3.} We now claim that $(a_n)^\infty_{n = 1}$ converges to the limit $a_0 \in A$. 

\vspace{6pt}

\textit{Verification.} Combining the above claim that $(a_n)^\infty_{n = 1}$ is cauchy and using the fact since $X$ is a Hilbert space, hence complete, it follows that $a_n \overset{n \rightarrow \infty} \rightarrow a_0 \in X$. But since $a_n \rightarrow a_0$, $a_n \in A$, and $A$ is closed, it follows that $a_0 \in A$. So we are done with \textit{Claim 3}. 

\vspace{6pt}

\textit{Claim 4.} We now claim the point $a_0 \in A$ in \textit{Claim 3} satisfies $\|x_0 - a_0 \| = \alpha$. 

\vspace{6pt}

\textit{Verification.} For all $n \in \bN$, from ($\diamondsuit$) we have $\alpha \le \|x_0 - a_0 \| \le \|x_0 - a_0 \| + \|a_n - a_0\| $.
Recall from ($\diamondsuit \diamondsuit$) we have $\|x_0 - a_0 \| \le \alpha + \frac 1 n$. So we get 
\[\|x_0 - a_0 \| + \|a_n - a_0\| < \alpha + \frac 1 n+ \|a_n - a_0\| \overset{n \rightarrow \infty} \rightarrow \alpha + 0 = \alpha\]

By applying the squeeze theorem we see $\|x_0 - a_0 \| = \alpha$, thus completing the proof.
\end{proof}

\vspace{6pt}

\begin{remark}\label{rem:24} 
In Theorem (~\ref{thm:23}), we note that the completeness of $X$ is an essential part to the hypothesis. 
\end{remark}

\newpage

\section{Orthogonal Projection onto a Closed Linear Subspace}

\subsection{May 15, Lecture 5}

\vspace{6pt}

\begin{remark}\label{rem:31} 
Note that for the normed vector space $\nvs$ over $\bR$, if we consider the closed linear subspace $W \subseteq X$, then $W$ is a non-empty, closed, convex set. 
\end{remark}

\vspace{6pt}

\begin{definition}\label{def:32}
Let $\ips$ be an inner product space. $x,y \in X$ are said to be \textit{orthogonal} to each other when they satisfy $\langle x,y \rangle = 0$, denoted 
\[x \perp y \]

If we consider $A,B \subseteq X$, we say that $A$ and $B$ are \textit{orthogonal} to each other to mean that
\[x \perp y, \forall x \in A, y \in B \]
\end{definition}

\vspace{10pt}

A few observations to point out would be that $x \perp y$ is the same as $y \perp x$, and we have for $x \in X$, $x \perp x \Leftrightarrow x = 0_X$. We also have the special case for $x \in X, B \subseteq X$, we write $x \perp B$ to mean that $x \perp y, \forall y \in B$. 

\vspace{6pt}

\begin{theorem}\label{thm:33}
Let $\ips$ be a Hilbert space, and suppose we're given a closed linear subspace, $W \subseteq X$, and an $x_0 \in X$. Further, define $\alpha := d_W(x_0) = \{ \| x_0 - w \| \; | \; w \in W \}$ and let $w_0$ be the unique point in $W$ such that $\| x_0 - w_0 \| = \alpha$.

Then, 
\[\label{eq:OP-Perp} \tag{OP Perp}  (x_0 - w_0) \perp W\]
\end{theorem}

\begin{proof}
...
\end{proof}

\vspace{6pt}

\textbf{Notation:}  For every $x_0 \in X$, let $\proj$ denote $w_0 \in X$ which is the unique point in $W$ who's at minimal distance from $x_0$, called the \textit{orthogonal projection} of $x_0$ into $W$. The name is used in connection to Theorem 3.3 which asserts that $(x_0 - \proj) \perp W$. 

\vspace{10pt}

\begin{definition}\label{def:34}
Let $\ips$ be a Hilbert space, and take $W$ to be a closed linear subspace of $X$. The \textit{orthogonal compliment} of $W$, $W^\perp$, is defined 
\[ W^\perp := \{ x \in X \; | \; x \perp W \} = \{ x \in X \; | \; x \perp w, \forall w \in W \} \]
\end{definition}

\noindent Note that $W^\perp$ is also a closed linear subspace. Hence, $W \leadsto W^\perp$ is an operation we do with closed linear subspaces of $X$. 

\vspace{10pt}

\subsection{May 17, Lecture 6} \textit{Do we know anything about the converse of Theorem (~\ref{thm:33})?}

\begin{remark}\label{rem:35}\textit{Converse to Theorem 3.3}
Let $\ips$, $W \subseteq X$, and $x_0 \in X$ be as in Theorem 3.3. Suppose we found a point, $w_0 \in W$, such that $(x_0 - w_0) \perp W$. Then, 
\[ \label{eq:OP-Metric} \tag{OP Metric} \|x_0 - w\| > \|x_0 - w_0\|, \; \forall w \in W \setminus \{w_0\} \]
\end{remark} 

\textit{Why is this?} For $w \not = w_0 \in W$ with $x_0 - w = (x_0 - w_0) + (w_0 - w)$, observe that $x_0 - w_0 \perp w_0 - w$. This is because $w_0 - w \in W$ and $(x_0 - w) \in W$. So, 
\[\|x_0 - w\|^2 = \| (x_0 - w_0) + (w_0 - w)\|^2 \overset{Pythag.} = \|x_0 - w_0\|^2 + \|w_0 - w\|^2 > \|x_0 - w_0\|^2\]

\vspace{6pt}
\noindent Since $\|w_0 - w\|> 0 $ as $w \not = w_0$. 
This implies $w_0$ must be the unique point in $W$ which is at minimal distance from $x_0$. 

\vspace{6pt}

Furthermore, we view $\proj$ as a function mapping from $X \rightarrow X$ which is a contractive linear operator on X (Shown in Question 4 of Homework Assignment 2). 

\vspace{6pt}

\begin{remark}\label{rem:36}
Recall our definition (~\ref{def:34}) for the \textit{orthogonal compliment} of $W$, $W^\perp$. We have that $W^\perp$ is also a closed linear subspace of $X$!

\vspace{6pt}

\textit{But is there a formula for $(W_1 \cap W_2)^\perp$?} For convenience, let's rewrite $W^\perp$ as 

\[ W^\perp = \bigcap_{w \in W} \underbrace{\left\{ x \in X \; | \; \langle x,w \rangle = 0 \right\}}_{:= \; Y_w} = \bigcap_{w \in W}Y_w\]

\noindent To check that $W^\perp$ is a closed linear subspace of $X$, it suffices to show each $Y_w$ is so.

We already know
\begin{itemize}[nosep, label= $\rightarrow$]
\item arbitrary intersections of linear subspaces are still linear subspaces, and
\item arbitrary intersections of closed sets are closed.
\end{itemize}

\noindent We are then left to fix a point $w \in W$, and check both

\[
\left.
\begin{array}{clc}
  0_X \in Y_w & &     \\
x,y \in Y_w & \Rightarrow & x + y \in Y_w \\
x \in Y_w, \alpha \in \bR & \Rightarrow & \alpha x \in Y_w 
\end{array}
\right\}
\; Y_w \mbox{ is a vector space of } X
\]
\vspace{6pt}
\[
\left.
\begin{array}{clc}
\begin{subarray}{c}
\mbox{If } (x_n)^\infty_{n=1} \in Y_w^\bN \\
\mbox{ with } x_n \overset{\| \cdot \|} \longrightarrow \; x \in X
\end{subarray}
 & \Rightarrow & x \in Y_w 
\end{array}
\right\}
\; Y_w \mbox{ is closed.}
\]
\end{remark}

\vspace{6pt}

\subsection{May 21, Lecture 7} In this lecture we introduced a third characterization of $P_W(x_0)$ by re-introducing a concept previously found in a course on Linear Algebra. 

\begin{proposition}\label{prop:37}
Let $\ips$ be a Hilbert space, and let $W \subseteq X$ be a closed linear subspace. Then, every $x \in X$ can be decomposed as a sum $x = w + y$, where $w \in W$, and $y \in W^\perp$. Moreover, this decomposition is unique. 
 \end{proposition}

\vspace{6pt}

\begin{proof} \textbf{Existence} Every $x \in X$ can be written as 
\[ x = \underbrace{P_W(x)}_{w} + \underbrace{x - P_W(x)}_{y}\] 
We have $w \in W$ by definition, and $y \perp w$ by ~\ref{eq:OP-Perp}, hence $ y \in W^\perp$. So we get $x = w + y$. \\

\noindent \textbf{Uniqueness} Suppose $x\in X$ is written as $x = w + y = w^\prime + y^\prime$ and $w,w^\prime \in W, y,y^\prime \in W^\perp$. 
Observe that $w + y = w^\prime + y^\prime \Rightarrow w-w^\prime = y^\prime - y =: z$. Thus we have $w-w^\prime \in W$, and $y^\prime - y \in W^\perp$, since $W,W^\perp$ are linear subspaces. So $z \in W \cap W^\perp = \{0_X\}$. Hence $z = 0_X$, therefore $w=w^\prime$, and $y^\prime = y$. 
\end{proof}

\newpage

\begin{remark}\label{rem:38} Proposition (~\ref{prop:37}) and its proof gives us another view on what is the orthogonal projection, $P_W(x)$. 
\[\label{eq:OP-3} \tag{OP 3} P_W(x) \mbox{ is the } w \mbox{ part in the unique decomposition } x = w + y \]
\end{remark}

\vspace{6pt}

\begin{proposition}\label{prop:39} Let $\ips$ be a Hilbert space, and $W \subseteq X$ be a closed linear subspace. Consider the linear operators $P_W:X\rightarrow X$, and $P_{W^\perp}:X\rightarrow X$. Then, 
\[P_W(x) + P_{W^\perp}(x) = x, \forall x \in X \]
(That is, $P_W + P_{W^\perp} = I$, where $I:X\rightarrow X$, is the idenity linear operator, $I(x) = x, \forall x \in X$) 
\end{proposition}

\begin{proof} ...
\end{proof}

\begin{corollary}\label{cor:310}
Let $\ips$ be a Hilbert space, and $W \subseteq X$ be a closed linear subspace. Then, 
\[ \left( W^\perp \right)^\perp = W \]
\end{corollary}

\section{Orthonormal Basis: \textit{For a seperable, infinite dimensional Hilbert space}}

\subsection{May 22, Lecture 8}

\begin{remark-and-notation}\label{remnot:41} Let $X$ be a vector space over $\bR$. Then for any subset, $S \subseteq X$, for the linear span of $S$ we use the notation span$(S)$. 

\vspace{4pt}

\textit{How do we describe span($S$)?} We use 2 conventions to characterize the span of $S$. 
\begin{description}
\item[Convention 1] 
\[ \sspan (S) := \left\{ x \in X 
	\left|
	\begin{array}{c}
	n \in \bN, x_1, \cdots, x_n \in S, \alpha_1, \cdots, \alpha_n \in \bR, \\
	\mbox{such that } x = \alpha_1 x_1 + \cdots  + \alpha_n x_n 
	\end{array}
	\right.
\right\}
\]
\vspace{2pt}
\item[Convention 2] We can also describe span($S$) as the smallest linear subspace of $X$ containing $S$. That is, we have 
\[ \begin{array}{rc}
i) & \sspan(S) \subseteq X, \mbox{and}\\
ii) & \begin{subarray}{c}
\mbox{If } V \subseteq X \mbox{ is a linear subspace of } X \\
\mbox{ such that } S \subseteq V, \mbox{ then } \sspan(S) \subseteq V
\end{subarray}
\end{array}\]
\end{description}

\vspace{4pt}

\noindent \textit{Why does $(ii)$ hold?} First note if $W$ is a linear subspace such that $W \supseteq S \Rightarrow W \supseteq \sspan(S)$. 
Now from PMATH351 we have $W$ closed and $W \supseteq \sspan(S) \Rightarrow W \supseteq \mbox{cl}(\sspan(S)) \supseteq \sspan(S)$.

\vspace{10pt}

 \textit{Stepping back into real analysis, we now introduce some new notation.}

\vspace{4pt}

\noindent Let $\nvs$ be a normed vector space over $\bR$. Then for any $S \subseteq X$, we denote
\[ \clspan(S) := \mbox{cl(span}(S)) \]

\vspace{6pt}

\noindent \textbf{Alternatively:} We can also characterize the clspan of $S$ in a similar fashion to span($S$), namely, it's now the smallest \textit{closed} subspace of $X$ containing $S$. 
\end{remark-and-notation}

\subsection{May 24, Lecture 9}

\vspace{6pt}

\begin{proposition}\label{prop:42}
Let $\nvs$ be a normed vector space over $\bR$. Then the following are equivalent.
\begin{enumerate}
\item $X$ is infinite dimensional and separable.
\item One can find an increasing chain of linear subspaces, $X_1 \subseteq \cdots \subseteq X_n \subseteq \cdots$, of $X$ such that $\dim(X_n) = n$ , and $\cup_{n=1}^\infty X_n$ is dense in $X$. 
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{description}
\item[$(1) \Rightarrow (2)$] Q5 on Homework Assignment 2
\item[$(2) \Rightarrow (1)$] Q1 on Homework Assignment 3
\end{description}
\end{proof}

\begin{remark}\label{rem:43}

Let $(x_n)_{n=1}^\infty$ be a sequence in $X$ such that $\forall n \in \bN$, its first $n$ elements, $x_1, \cdots, x_n$, form a linear basis in $X$. 
Let $S = \{x_1, \cdots, x_n, \cdots\} \subseteq X$, which has no repetitions, to mean $x_i \not = x_j, \forall i \not = j \in \bN$, due to the linear independence of $x_1, \cdots, x_n$ when $n > \max\{i,j\}$. \textbf{Observe:} that $\clspan(S) = X$. 

\vspace{4pt}

Indeed, $\forall n \in \bN$ we have $\sspan(S) \supseteq \sspan(x_1, \cdots, x_n) = X_n$. Hence, $\sspan(S) \supseteq \cup_{n=1}^\infty X_n$, and therefore, 
\[ \clspan(S) = \mbox{cl}(\sspan(S)) \supseteq \cl(\cup_{n=1}^\infty X_n) = X \]
\[\Rightarrow \clspan(S) = X \]

The sequence $\seq$ with this property is said to be a \textit{total sequence} in $X$. \\
\noindent \textbf{Warning:} This is not saying $S$ is dense, but rather that its span is dense in $X$. 

\vspace{6pt}

In the setting up of Proposition ($~\ref{prop:42})$, all $X_n$'s are sure to be closed linear subspaces. 
\begin{itemize}[ label= $\rightarrow$]
\item In general, for a normed vector space over $\bR$, $\nvs$, with $V \supseteq X$, we have that \[\dim(V) < \infty \Rightarrow V \mbox{ is closed. } \]
\item This is because $V$ is complete in the metric associated to $\| \cdot \|$, which is in turn a consequence of the EVT. 
\end{itemize}
\end{remark}

\vspace{6pt}

\subsection{May 27, Lecture 10}

\begin{proposition} \label{prop:44}
Let $\ips$ be a Hilbert space over $\bR$. Assume $X$ is both separable and infinite dimensional. Let $\incrchn$ be linear subspaces in $X$ (as in Proposition ~\ref{prop:42}). Then, we can find a sequence, $(\xi_n)^\infty_{n=1}$ in $X$ such that 
\begin{itemize}[label={}]
\item \[ \tag{\ding{46}} \sspan(\xi_1, \cdots, \xi_n) = X_n, \forall n \in \bN \]
\item \[\tag{\ding{46}\ding{46}} \xi_i \perp \xi_j, \forall i \not = j \in \bN	\]
\item	\[\tag{\ding{46}\ding{46}\ding{46}} \|\xi_i\| = 1, \forall i \in \bN \]
\end{itemize}
We refer to (\ding{46}\ding{46}) and (\ding{46}\ding{46}\ding{46}) by saying $\onseq$ is an orthonormal sequence in $X$. 
\end{proposition}

\begin{proof}
Question 1 (a) in Homework Assignment 3 provides us with a sequence $\seq$ in $X$ such that for every $n \in \bN$, the vectors $x_1, \cdots, x_n$ form a linear basis for $X_n$. We will use Gram-Schmidt to convert the $x_n$'s into an orthonormal sequence. Formally, we proceed by induction on $n$. 

\newpage

\noindent\textbf{Base Case:} Put $\xi_1 = \frac 1 {\|x_1\|} x_1 \in X_1$. Note that $\|x_1\| \not = 0$ since $X_1$ has dimension 1, of which $x_1$ forms the basis. \\

\noindent \textbf{Inductive Step:} Suppose for some $n \ge 1$, we have constructed $\xi_1, \cdots, \xi_n$ such that $\|\xi_1\|=\cdots = \|\xi_n\| = 1$, $\xi_i \perp \xi_j, \forall 1 \le i < j \le n$, and $\sspan(\xi_1, \cdots, \xi_n) = X_n = \sspan(x_1, \cdots, x_n)$. We look at $X_{n+1} = \sspan(x_1, \cdots, x_n, x_{n+1}$) and put $\eta := x_{n+1} - (t_1\xi_1 + \cdots + t_n\xi_n)$, with $t_i = \langle x_{n+1},\xi_i\rangle, 1\le i \le n$. The following claims are used complete the proof. 
\begin{itemize}
\item $\eta \not = 0_X$
\item $\langle \eta, \xi_i \rangle = 0, \forall 1 \le i \le n$, and
\item $\sspan(\xi_1, \cdots, \xi_n, \eta) = \sspan(x_1, \cdots, x_n, x_{n+1}) = X_{n+1}$. \\
\end{itemize}

\noindent Finally, put $\xi_{n+1} = \frac 1 {\|\eta\|} \eta$ since $\|\eta\| \not = 0$ by claim 1, and observe $\|\xi_{n+1}\| = 1$. So we get 
\[ \langle \xi_{n+1}, \xi_i \rangle = \frac 1 {\|\eta\|} \langle \eta, \xi_i \rangle \overset{claim \; 2}= 0, \forall 1 \le i \le n.\]
So, $\sspan(\xi_1, \cdots, \xi_{n+1}) = \sspan(x_1, \cdots, x_{n+1})$, completing the proof. 
\end{proof}

\vspace{6pt}

\begin{example}\label{ex:45}
Consider the space $ell^2$ as in Homework Assignment 1, and for an increasing chain of $X_n$'s, use $X_n = \{(x^{(1)}, \cdots, x^{(n)}, 0, \cdots) \: | \; x{(k)} \in \bR, 1 \le k \le n \}$. 
\end{example}

\vspace{6pt}

\begin{definition}\label{def:46}
Let $\ips$ be a separable, infinite dimensional Hilbert space on $\bR$. A sequence satisfying Proposition ~\ref{prop:44} is said to be an \textit{orthonormal basis} for $\ips$.
\end{definition}

\vspace{10pt}

\section{Coefficients with respect to an Orthonormal Basis}

\noindent For the remainder of the chapter, we fix a separable, infinite dimensional Hilbert space, $\ips$, and an orthonormal basis, $\onseq$, for $X$. 

\subsection{May 30, Lecture 11}

\begin{definition}\label{def:51}
$\forall x \in X$, the sequence $\left( \langle x, \xi_n \rangle \right)^\infty_{n=1}$ is (for now) called a \textit{sequence of $\xi$-coefficients of $x$}.\footnote{This is in relation to \textit{Fourier coefficients}, which we have yet to "discover".}
\end{definition}

\vspace{6pt}

\begin{remark}\label{rem:52} \textit{Given this sequence, can we get $x$ back?} \\Let $B:= \{\xi_1, \cdots, \xi_n, \cdots\} \subseteq X$, which has no repetitions \textit{(since they're a basis, hence linearly independent)}. \\

Observe that $\clspan(B) = X$. Indeed, in terms of our notation with $X_n = \sspan(\xi_1, \cdots, \xi_n)$, we get $\sspan(B) = \cup_{n=1}^\infty X_n \Rightarrow  \clspan(B) = \clspan(\cup_{n=1}^\infty X) = X$. 
As a concequence, if $z \in X$ such that $z \perp \xi_i, \forall i \in \bN$, it follows that $z = 0_X$. 

\noindent Indeed, $z \perp \xi_n, \forall n \in \bN \Rightarrow z \in B^\perp \Rightarrow B^\perp \overset{ Hwk A3, Q2} = (\clspan(B))^\perp = X^\perp = \{ 0_X \}$. So $z = 0_X$.%So $z \in \{0_X\}$. 
\end{remark}

\vspace{6pt}

\begin{proposition}\label{prop:53}
If $x, x^\prime$ have the same sequence of $\xi$-coefficients, then $x = x^\prime$. 
\end{proposition}

\begin{proof}
...
\end{proof}

\vspace{6pt}

\begin{theorem}\label{thm:54} \textit{Rieze-Fisher Theorem} \\
Take $x \in X$, and define the sequence of $\xi$-coefficients,$(c_n)^\infty_{n=1}$ of $x$. For every $n \in \bN$, let $x_n := c_1 \xi_1 + \cdots + c_n x_n$. Then, 
\begin{enumerate}
\item \[ \forall n \in \bN, x_n = P_{X_n}(x) \]
\item \[ \lim_{n\rightarrow \infty} \|x_n - x\| = 0\]
\end{enumerate}
\end{theorem}
Note that $P_{X_n}$ is the orthogonal projection onto $X_n$ defined as in Proposition ~\ref{prop:44}. 

\subsection{June 1, Lecture 12}

\begin{remark}\label{rem:55} \textit{Interpretation of Rieze-Fisher}

We can rephrase $(1)$ as $x_n := \sum_{n=1}^\infty c_i \xi_i$. Then we get $x_n \overset{\|\cdot\|}\longrightarrow x$, that is,
\[ \|\cdot\|-\lim_{n\rightarrow \infty} \left( \sum_{i=1}^\infty c_i \xi_i \right) = x\]

This equation can be written as the $\|\cdot\|$-convergent series, 
\[\label{eq:R-F} \tag{R-F} x = \sum_{n=1}^\infty c_i \xi_i \]
\end{remark}

\begin{proposition}\label{prop:56}
Using the framework as above, pick $x \in X$, and let $(c_n)^\infty_{n=1}$ be its sequence of $\xi$-coefficients. Then, 
\[ \label{eq:Parseval} \tag{Parseval} \|x\|^2 = \sum_{i=1}^\infty c_i^2\]
\end{proposition}

\begin{proof}
...
\end{proof}

\begin{remark}\label{rem:57} \textit{How to interpret Parseval's formula}\\
Parseval says a vectors sequence of $\xi$-coefficients $c = (c_1, \cdots, c_n, \cdots) \in \ell^2,$ and $\|c\|^2_{\ell^2} = \|x\|^2$. Thus, we can find a function $\varphi: X\rightarrow \ell^2$. 

\[ \tag{$\ell^2$-iso} \varphi(x) = (\langle x, \xi_1 \rangle, \cdots, \langle x, \xi_n \rangle, \cdots)	\]
\end{remark}

\textbf{Properties of $\varphi$:} \begin{enumerate}
\item It's immediate to see that $\varphi$ is linear
\item ~\ref{eq:Parseval} says $\varphi$ is an isomorphic linear map. This gives us the notation for an isomorphic linear map between two Hilbert spaces:
\[ \| \varphi(x)\|_{\ell^2} = \|x\|_X, \forall x \in X \]
\item Our map $\varphi:X \rightarrow \ell^2$ tells us $\varphi(\xi_n) = e_n$, with $e_n = (0, \cdots, 0,1,0,\cdots)$, the standard basis vector. 
\item $\varphi:X \rightarrow \ell^2$ is bijective, hence its what one calls a Hilbert space isomorphism. \\
\end{enumerate}

\noindent \textbf{Moral of studying $\varphi$:} We get $X \approx \ell^2$, an isomorphic hilbert space. Hence, any two infinite dimensional, separable Hilbert spaces are isomorphic to each other ($X \approx \ell^2 \approx Y$). The precise statement $X \approx Y$ is in Homework Assignment 4, Question 4. \\

%This concludes Part 1 of the course. 

\part{The Lebesgue Measure}


\newpage
\begin{thebibliography}{99}

\bibitem{BBT1997}  
Andrew M. Bruckner, Judith B. Bruckner, Brian S. Thomson.
{\em Real Analysis}, Prentice-Hall Publishers, 1997.

\vspace{4pt}

\bibitem{PMath450-S2024} A. Nica.
Lecture summaries for PMath 450 lectures in Summer Term 2024,
available on the Learn web-site of the course.

\vspace{4pt}

\bibitem{HLR1988} H.L. Royden. {\em Real Analysis, 3rd edition}, Prentice-Hall Publishers, 1988.
\end{thebibliography}


\end{document}

